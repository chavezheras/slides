---
title: ISSA-FRA-masterclass
draft: false
tags:
  - presentation
date_created: 06 July 2025
date_modified: 07 July 2025
margin: 0
width: 1920
height: 1200
transition: slide
theme: black
date: 06 July 2025
---
<section data-background-transition="zoom" data-background-video="https://github.com/chavezheras/slides/raw/main/assets/machine_vision_BG.mp4"
          data-background-video-loop data-background-video-muted data-background-opacity=.2>
          <aside class="notes">
          <p>Gracias Juan Luis por la invitaci√≥n, encantado de estar aqu√≠.</p>
          </aside>
</section>


# ISSA
## Sistemas Inteligentes para Archivos Audiovisuales 



![[assets/images/db3a81949ab652d55054d76b287e03fa_MD5.jpg|250]]

[Dr Daniel Ch√°vez Heras](https://movingpixel.net/)

[movingpixel.net ](https://movingpixel.net/) | [@dchavezheras.bsky.social‚Ä¨](https://bsky.app/profile/dchavezheras.bsky.social)

Fundaci√≥n Ram√≥n Areces, Madrid 2025

---

<!-- slide bg="#2b1804" -->

> [!example]  En los siguientes 45 minutos:  üëáüèº  
> 1. ¬øQu√© y por qu√© ISSA?
> 2. Antecedentes
> 3. Ejemplos
> 4. Producci√≥n cultural en la era de la IA


---

<!-- slide bg="#304f5e" -->

![](fas fa-cubes fa-5x) <!-- .element: color="teal" -->

# 1. ¬øQu√© y por qu√© ISSA?


---

![[ISSA_1.png]]

note:
En qu√© consiste el proyecto, sus socios y colaboradores, financiamiento, objetivos, y el programa de actividades.

---

<grid  drag="80 60" drop="center" flow="col">


> [!info]  ISSA - objetivos
> - Impulsar **experimentaci√≥n creativa con tecnolog√≠as de IA** para la preservaci√≥n, acceso, y activaci√≥n del patrimonio cultural audiovisual.
> 
> - Desarrollar conocimiento, herramientas y habilidades para **redise√±ar acervos audiovisuales** desde una perspectiva computacional interdisciplinaria.
> 
> - Evaluar de manera cr√≠tica el potencial de diversas tecnologias de IA para **crear nuevos modos de interacci√≥n  y valor p√∫blico** en organizaciones culturales como archivos de cine y televisi√≥n.
</grid>

---
<!-- slide bg="#f4f4f4" -->
![[BFI ONE LOTTERY_MONO POS (1).png]]

---
<!-- slide bg="#ffffff" -->

![[Pasted image 20250707125920.png|900]]


![[assets/images/ISSA_partners.png|1000]]

note:

> [!info] ISSA partners 
> - [North West Film Archive](https://kcl-research.worktribe.com/record.jx?recordid=3782417)
> - [National Library of Scotland](https://kcl-research.worktribe.com/record.jx?recordid=3392648)
> - [Yorkshire Film Archive](https://kcl-research.worktribe.com/record.jx?recordid=3398513)
> - [Northern Ireland Screen Commission](https://kcl-research.worktribe.com/record.jx?recordid=3398515)
> - [National Library of Wales](https://kcl-research.worktribe.com/record.jx?recordid=3398549)
> - [Film Archives UK](https://kcl-research.worktribe.com/record.jx?recordid=3782422) (convening partner)

UK scope, one archive in each country. 

---

<!-- slide bg="#ffffff" -->

![[assets/images/2025-03-05_issa-kdl/Slide1.jpg|1300]]
https://kdl.kcl.ac.uk/‚Äã

note:
https://kdl.kcl.ac.uk/about/team/ ‚Äã
https://kdl.kcl.ac.uk/projects/research-themes/
https://kdl.kcl.ac.uk/faqs/ 

King‚Äôs Digital Lab (KDL) is una unidad de la universidad dedicada a la Ingenier√≠a de software para investigaci√≥na (Research Software Engineering). Es partre de la Facultad de Artes y Humanidades. 
- 100 Digital Humanities projects collected over the past two decades 
- ~20 projects in active development
- ~60+ sixty projects in various post-funding states, con HEIs, GLAM, industrias creativas

The role of KDL is to design and deliver the core research and development provision for the ISSA, including applied expertise in machine learning in cultural domains. Learn more about King‚Äôs Digital Lab by browsing some of its [recent projects](https://kdl.kcl.ac.uk/projects/).

---

<!-- slide bg="#ffffff" -->

![[ISSA_summary.png]]

note:

Daniel
### Aims and objectives 
To drive **creative experimentation with AI technologies** in the screen heritage sector by developing the knowledge, tools, and skills needed to rethink large audiovisual collections from a computational perspective. 

ISSA aims to build understanding of AI technologies that are relevant for moving image archives and to explore the potential of these technologies to add public value to screen heritage organisations in their wider contexts.

To do this, we will:

1. Develop a prototype for creative experimentation with moving image collections, including modules for data enrichment, exploration, retrieval, and interaction (DEERIN); 
2. Co-design of situated experimentation workshops delivered through a format called AI for Media Sandbox (AIMS); 
3. Create a publicly accessible code repository and knowledge base for archives to document these experiments and share tools, knowledge and best practice that arises from them; and 
4. Document requirements and sector gaps that can be used to attract future funding and inform strategic decisions about AI in moving image archives. 

---

<!-- slide bg="#ffffff" -->

![[ISSA_roadmap.png]]

---


![[assets/images/ISSA_web_snap.png|1000]]
[BFI press release](https://core-cms.bfi.org.uk/media/39909/download "https://core-cms.bfi.org.uk/media/39909/download")  ¬∑  [ISSA website](https://www.kcl.ac.uk/research/issa "https://www.kcl.ac.uk/research/issa")  

---

![[ISSA_FAQ 1.png|1000]]
[ISSA code repository and WIKI](https://github.com/kingsdigitallab/issa/wiki/Frequently-Asked-Questions)

---

<!-- slide bg="#304f5e" -->
![](fas fa-square fa-5x) <!-- .element: color="teal" -->

# 2. Antecedentes


---


![[media/anatomy_of_ai.png|1500]]

note:
This was a project in experimental television, in hindsight it was very much of its time, and frankly, as time goes by I am more and more surprised that no one stopped us from doing it. 

In light of everything that happened after with AI, this project stands a somewhat of a quaint expression of many of the issues discussed yesterday, from access and copyright, to the role of generative technologies and their impact on screen culture today.

---


![[media/183e9e0918ae84bbdba0e566dd06a5e7_MD5.jpg|1200]]

note:
A project in 2018 to "machine-see" the BBC television archive, and "machine-edit" new sequences "television by the meter" ‚Äïwe jokingly referred to it.

Aired on BBC 4 on September 2018, seen by half a million people in the UK.
It included four sections that corresponded to different computational techniques to traverse the archive: including text analysis over subtitled material, object detection, motion estimation (visual energy), and a mixed between the three (what we would call today a multimodal method).

---

<iframe src="https://player.vimeo.com/video/429123060?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" width="1920" height="1080" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media" title="1. Made by Machine: Introduction"></iframe>
note:
The possibilities and limitations of AI models in the context of moving image archives.
Analysis = generation.

---

<split even gap="3">

![[media/599f364553168e0448c2566cb33d0c94_MD5.png|700]]

![[assets/images/bce88512ea3044d00c7c54c1897bade9_MD5.png|Open: CMV_contents.png|700]]


</split>

note:
In my book I focus on moving images: artefacts in and of motion that structure cognitive and affective responses in their audiences.

The first two moments: two chapter from my new book in which I discuss philosophies of time and temporal dynamics in moving images from a computational perspective. 
I will present a very condensed version the these chapters as a theoretical foundation, as well as ongoing research of how these concepts can be applied in practice.

---
<section data-background-video="https://github.com/chavezheras/slides/raw/main/assets/Grandmas_Reading_Glass_1900.mp4"
          data-background-video-loop data-background-video-muted>
    <aside class="notes">
          <p>One part of the answer is that this ability to watch films was learned over time. It was not always the case that we knew how to interpret a close up, for example. On screen you see one of the earliest recordings of a close ups in a film. 
		<p>_Grandma's Reading Glass_ (c.1900) by George Albert Smith.
		<p>At the time, the close up was a technical novelty. These early films are demonstrations of technology more than anything else. Through social exposure and continued use over time, techniques like the close up became formal conventions ‚Äï the pieces in a larger aesthetic apparatus of cinema.
     </aside>
</section>

---


![[media/71efe22f312e3f76e247f72f0e7f3e00_MD5.png|1500]]

note:
Using computer vision, we can enlist computers to see these images in our behalf to find some of the patterns that emerge from conventional narrative techniques. For example the shot-reverse-shot, commonly used to depict conversations between characters. Using face detection, it is possible to get a sense of how cinematic discourse is constructed through editing: from wider shots that establish the relations between characters and their environment, progressively in...

---

![[media/95145a7104d332087db98530f2d0c248_MD5.png|1500]]


note:
...to close ups that show characters' inner states, intentions, and reactions, through their facial expressions. David Bordwell calls this editing style from the general to the particular _analytical editing_.

---
## Distribuci√≥n de duraciones de planos

![[media/shot_histogram.jpeg|1200]]

note:
conventional statistics give overview of this dataset of shots

---
## C√°lculo de escala de planos

![[media/schot_scale_detector.jpg|1500]]

---

## Distribuci√≥n de escalas de planos

![[media/shot_scale_chart.jpeg|1200]]

note:
more sophisticated tools to do this now, like Cineshot deep learning method to find shot scale in films

---

![[media/46ff8796887a813b083c83026462362d_MD5.png]]

note:
At the same time, we admit film's capacity to dilate and compress time at the level of shots and sequences. The interplay between these two properties makes films both structured and elastic, mechanistically bound but expressively designed. 

At this micro level, we can recognise at least two types of differences: strong higher but small differences between continuous frames, and weak but larger differences in contiguous shots. High frequency, small variation vs lower frequency and higher variation.

---

### Burch Computacional

![[assets/images/Pasted image 20241015225413.png]]

note:
Temporal structure in film is usually broken down analytically into sequences, scenes, and shots. These are units with which filmmakers and film editors design an intelligible whole from a pool of possible parts, most often recorded in various locations at separate times and in a different order.
Bong Joon-¬≠Ho and Herzog

With a background in music, Burch classifies the ways in which a film can be edited by identifying all possible space-¬≠time articulations that describe a minimal relation that exists between any two consecutive shots, like the ‚Äònotes‚Äô of a film.

A set of formal ‚Äòobjects‚Äô ‚Äì the fifteen different types of shot transitions and the
parameters that define ¬≠them ‚Äì capable of rigorous development through such devices as rhythmic alternation, recapitulation, retrogression, gradual elimination, cyclical repetition, and serial variation, thus creating structures similar to those of
twelve-¬≠tone music. (Burch, 1981, p. 14)

---

![[assets/images/Pasted image 20241015225539.png]]

---

<split even gap="3">

![[assets/images/Pasted image 20241015225745.png|750]]

![[assets/images/Pasted image 20241015225830.png|750]]

</split>

note:
difference in time and space articulations

---
### Experimentos de visualizaci√≥n de montaje

![[assets/images/Pasted image 20241016000527.png|1700]]
Image credit: [Ryan Heuser](https://www.cdh.cam.ac.uk/about/people/dr-ryan-heuser/)

---

![[assets/images/Pasted image 20241016000458.png|1700]]
Image credit: [Ryan Heuser](https://www.cdh.cam.ac.uk/about/people/dr-ryan-heuser/)

---

![[assets/images/Pasted image 20241016000419.png|1700]]
Image credit: [Ryan Heuser](https://www.cdh.cam.ac.uk/about/people/dr-ryan-heuser/)

---

![[assets/images/Pasted image 20241016000617.png|1700]]
Image credit: [Ryan Heuser](https://www.cdh.cam.ac.uk/about/people/dr-ryan-heuser/)

---

<!-- slide bg="#304f5e" -->
![](fas fa-cubes fa-5x) <!-- .element: color="teal" -->

# 3. Ejemplos

---

<grid  drag="80 60" drop="center" flow="col">


> [!success] El prototipo DDERIn busca:
> - Facilitar la experimentaci√≥n √°gil con modelos existentes de IA, en el contexto de acervos audiovisuales, en sus contextos institucionales y applicado a sus colecciones.
> - Construir una base de conocimiento t√©cnico y evidencia para que archivos audiovisuales decidan si, y como utilizar tecnolog√≠as de IA en sus organizaciones.

> [!error] El prototipo DDERIn no pretende:
> - Desarrollar una plataforma, producto o servicio listo para mercado.
> - Aportar en todas las areas de preservacion y acceso del patrimonio audiovisual.
> - Entrenar nuevos models desde cero.

</grid>

note:
DDERIn es un modelo tecnico y conceptual para investigacion y desarollo.

---

<grid  drag="80 60" drop="center" flow="col">
> [!iexample] Aplicaciones
> 1. **Enriquecimiento**: generaci√≥n de datos y metadatos para cat√°logo y acceso mediante LLMs y LVMs.
> 2. **Exploraci√≥n**: Navegaci√≥n y visualisaci√≥n de acervos mediante t√©cnicas de proyeccion y clustering (t-SNE, UMAP).
> 3. **B√∫squeda**: T√©cnicas de b√∫squeda y recuperaci√≥n semantica y multimodal (RAG, graphRAG).
> 4. **Interacci√≥n**: aplicaciones e interfaces para reuso creativo.
</grid>



note:
The proposed DEERIN prototype will be designed as a modular system, using state-of-the-art open-source libraries and models, to provide a functional blueprint of how to enrich, explore and interact with a large collection of moving images.

DEERIn is not a finished product or service, it is a tool for research and a technical blueprint for guided experimentation. We will not address every aspect of archival practice. We will not train new models from scratch. We will enable archives to experiment with existing state of the art open-source models in their own collections and contexts, to build the knowledge and evidence to decide if and how to use and develop AI products and services.

The complementary AIMS workshops are designed to situate the prototype in different contexts, with budgeted time and resources to identify, sample and reshape collection asynchronously, and focus on creative experimentation and discussion in person. 

---

<grid  drag="80 60" drop="center" flow="col">
> [!example]  Ejemplos
> 1. Segmentaci√≥n sem√°ntica semi-automatizada 
> 2. Generaci√≥n automatizada de audio descriptivo
> 3. B√∫squeda y recuperaci√≥n multi modal
> 4. Edici√≥n semi-automatizada para reuso creativo
</grid>

---

<!-- slide bg="#5E4031" -->

![](fas fa-cube fa-5x) <!-- .element: color="#91634C" -->

## 3.1 Segmentaci√≥n sem√°ntica semi-automatizada
---

## Experimentos con videocintas de H2022


<split even gap="2">

![[assets/images/a122d80981e384b79764ad1bf5a59ffb_MD5.png]]

<iframe title="vimeo-player" src="https://player.vimeo.com/video/1099640200?h=5f5805e1d4" width="640" height="360" frameborder="0"    allowfullscreen></iframe>

</split>


---

<!-- slide bg="#5E4031" -->
![](fas fa-cube fa-5x) <!-- .element: color="#91634C" -->

## 3.2 Generaci√≥n automatizada de audio descriptivo

---

## DANTE-AD

![[assets/images/b9495677f4e92c63797cc5d5076de55d_MD5.png]]
[Adrienne Deganutti](https://www.linkedin.com/in/adrienne-deganutti-bb28031b6/),¬†[Simon Hadfield](https://www.surrey.ac.uk/people/simon-hadfield),¬†[Andrew Gilbert](https://andrewjohngilbert.github.io/)

C-CATS Lab University of Surrey

---

![[assets/images/8d9e2328ea5c762dc340bb81396b537e_MD5.png]]
[Adrienne Deganutti](https://www.linkedin.com/in/adrienne-deganutti-bb28031b6/),¬†[Simon Hadfield](https://www.surrey.ac.uk/people/simon-hadfield),¬†[Andrew Gilbert](https://andrewjohngilbert.github.io/)

C-CATS Lab University of Surrey

---

<!-- slide bg="#5E4031" -->

![](fas fa-cube fa-5x) <!-- .element: color="#91634C" -->

## 3.3 B√∫squeda y recuperaci√≥n multi modal

---

## WISE-2

![[assets/images/8547da2885922f896c0d0aee0559e6a0_MD5.png|1000]]
Motor de b√∫squeda multimodal [WISE](https://www.robots.ox.ac.uk/~vgg/software/wise/)

[Abhishek Dutta](mailto:adutta@robots.ox.ac.uk) [Visual Geometry Group (VGG)](https://www.robots.ox.ac.uk/~vgg/) University of Oxford.

note:
https://www.robots.ox.ac.uk/~vgg/software/wise/
https://meru.robots.ox.ac.uk/wise/pass/

---

<!-- slide bg="#5E4031" -->
![](fas fa-cube fa-5x) <!-- .element: color="#91634C" -->

## 3.4 Edici√≥n semi-automatizada para reuso creativo


---


![[assets/images/f147e0b337ed2330668fd23c568251d0_MD5.png]]
K1 Adobe Premiere plugin para autoedici√≥n
[KasparAI](https://kasparai.com/)

note:
K1 Adobe premier plugin for prom-editing

---
![[Pasted image 20250710230534.png]]
[PlayPhrase.me](https://www.playphrase.me/)

---

![[assets/images/d9186f5ef3c80baab006b20a3d3d5143_MD5.png]]
[FrameSense](https://github.com/kingsdigitallab/framesense)

---


![[assets/images/eb246b12842e74b89e92a268cfd00055_MD5.png]]
[FrameSense](https://github.com/kingsdigitallab/framesense)

---

<!-- slide bg="#304f5e" -->
![](fas fa-border-none fa-5x) <!-- .element: color="teal" -->

# 4. Producci√≥n cultural en la era de la IA

---



<grid drag="55 20" drop="1">
> - Razonamiento deductivo
> - Realismo estructural
> - Relaciones de causalidad
> - Mapeos y taxonom√≠as
</grid>

<grid drag="55 20" drop="-1">
> - Razonamiento inductivo
> - Realismo relacional
> - Relaciones de contig√ºidad
> - Modelos y simulaciones
</grid>



---
<iframe width="1080" height="768" src="https://www.youtube.com/embed/CNIlqJctA_I?si=hassiNEby7XizlFS" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
note:
El medio es el mensaje. Esto era cierto del radio, el cine y la television.
El medio no es el artefacto, ni sus usuarios, sino la red de relaciones que se construyen cuando artefactos y audiencias entren en contacto. La diferencia clave con tecnolog√≠as de IA es que esta red invisible de relaciones ha sido capturada y abstra√≠da en modelos comunicacionales capaces de reproducir la potencialidad de esta red de relaciones.
El reto con la producci√≥n cultural en la era de la IA es que estos modelos de potencialidades latentes modifican radicalmente el modo de producci√≥n, preservaci√≥n, y acceso, a la cultura.

Benjamin Desai is a creative technologist and digital artist focused on AR and cofounder of Radical Realities, an award-winning immersive studio.  ‚ÄúExcited to share this imaginative look into an alternate past powered by Sora. Blending early 20th-century film aesthetics with whimsical scenarios and placing animals in unexpected roles. This work aims to ignite a sense of wonder while showcasing the potential of today's technology. Creating with Sora is still an experimental process, involving a lot of iteration and fine-tuning. It's much more of a human-AI collaboration than a magic button solution. Check out these fascinating animals and the joy of asking 'what if?'‚Äù

Analytic and generative approaches in computing tend to be split between scientific and creative domains, with their respective tools and communities of practice.

Deployed as analytical engines, computers can be used to find patterns across vast collections of imagery, and these patterns are often expressed as relations of proximity in space. We have seen examples of this yesterday in multiple way of projecting archives onto 2d or 3d spaces.

The image in this slide shows a t-SNE mapping of a collection of soviet news reels. The dataset and the tool were developed by colleagues in CUDAN from the cultural data analytics lab in Tallinn, Estonia.

But to amount to knowledge, these spatial correlations require interpretation and explanation, which require relations of necessity, not just proximity, and tend to unfold sequentially, as researchers, critics and users seek to organise these patterns to infer causal relations and plausible reasons for data objects and events to be organised in space the way they are.

By coupling an analytical engine with a generative one, computing can be used to configure narratives about these proximity patterns and enable explanatory propositions through compositional techniques familiar to media scholarship.
