---
title: ISSA-FRA-masterclass
draft: false
tags:
  - presentation
date_created: 06 July 2025
date_modified: 07 July 2025
margin: 0
width: 1920
height: 1200
transition: slide
theme: black
date: 06 July 2025
---
<section data-background-transition="zoom" data-background-video="https://github.com/chavezheras/slides/raw/main/assets/machine_vision_BG.mp4"
          data-background-video-loop data-background-video-muted data-background-opacity=.2>
          <aside class="notes">
          <p>Gracias Juan Luis por la invitaci√≥n, encantado de estar aqu√≠.</p>
          </aside>
</section>


# ISSA
## Sistemas Inteligentes para Archivos Audiovisuales 



![[assets/images/db3a81949ab652d55054d76b287e03fa_MD5.jpg|250]]

[Dr Daniel Ch√°vez Heras](https://movingpixel.net/)

[movingpixel.net ](https://movingpixel.net/) | [@dchavezheras.bsky.social‚Ä¨](https://bsky.app/profile/dchavezheras.bsky.social)

Fundaci√≥n Ram√≥n Areces, Madrid 2025

---

<!-- slide bg="#2b1804" -->

> [!example]  En los siguientes 45 minutos:  üëáüèº  
> 1. ¬øQu√© y por qu√© ISSA?
> 2. Antecedentes
> 3. Estudios de caso
> 4. Producci√≥n cultural en la era de la IA
> 5. Preguntas

---

<!-- slide bg="#304f5e" -->

![](fas fa-tv fa-5x) <!-- .element: color="teal" -->

# 1. ¬øQu√© y por qu√© ISSA?


---

![[ISSA_1.png]]

note:
En qu√© consiste el proyecto, sus socios y colaboradores, financiamiento, objetivos, y el programa de actividades.

---

<grid  drag="80 60" drop="center" flow="col">


> [!info]  ISSA - objetivos
> - Impulsar **experimentaci√≥n creativa con tecnolog√≠as de IA** para la preservaci√≥n, acceso, y activaci√≥n del patrimonio cultural audiovisual.
> 
> - Desarrollar conocimiento, herramientas y habilidades para **redisedise√±ar acervos audiovisuales** desde una perspectiva computacional interdisciplinaria.
> 
> - Evaluar de manera cr√≠tica el potencial de diversas tecnologias de IA para **crear nuevos modos de interacci√≥n  y valor p√∫blico** en organizaciones culturales como archivos de cine y televisi√≥n.
</grid>

---
<!-- slide bg="#f4f4f4" -->
![[BFI ONE LOTTERY_MONO POS (1).png]]

---
<!-- slide bg="#ffffff" -->

![[Pasted image 20250707125920.png|1200]]

---

<!-- slide bg="#ffffff" -->

![[assets/images/ISSA_partners.png|1200]]

note:

> [!info] ISSA partners 
> - [North West Film Archive](https://kcl-research.worktribe.com/record.jx?recordid=3782417)
> - [National Library of Scotland](https://kcl-research.worktribe.com/record.jx?recordid=3392648)
> - [Yorkshire Film Archive](https://kcl-research.worktribe.com/record.jx?recordid=3398513)
> - [Northern Ireland Screen Commission](https://kcl-research.worktribe.com/record.jx?recordid=3398515)
> - [National Library of Wales](https://kcl-research.worktribe.com/record.jx?recordid=3398549)
> - [Film Archives UK](https://kcl-research.worktribe.com/record.jx?recordid=3782422) (convening partner)

UK scope, one archive in each country. 


---

<!-- slide bg="#ffffff" -->

![[assets/images/2025-03-05_issa-kdl/Slide1.jpg|1300]]
https://kdl.kcl.ac.uk/‚Äã

note:
https://kdl.kcl.ac.uk/about/team/ ‚Äã
https://kdl.kcl.ac.uk/projects/research-themes/
https://kdl.kcl.ac.uk/faqs/ 

King‚Äôs Digital Lab (KDL) is una unidad de la universidad dedicada a la Ingenier√≠a de software para investigaci√≥na (Research Software Engineering). Es partre de la Facultad de Artes y Humanidades. 
- 100 Digital Humanities projects collected over the past two decades 
- ~20 projects in active development
- ~60+ sixty projects in various post-funding states, con HEIs, GLAM, industrias creativas

The role of KDL is to design and deliver the core research and development provision for the ISSA, including applied expertise in machine learning in cultural domains. Learn more about King‚Äôs Digital Lab by browsing some of its [recent projects](https://kdl.kcl.ac.uk/projects/).

---


<!-- slide bg="#f4f4f4" -->

![[ISSA_summary.png]]

note:

Daniel
### Aims and objectives 
To drive **creative experimentation with AI technologies** in the screen heritage sector by developing the knowledge, tools, and skills needed to rethink large audiovisual collections from a computational perspective. 

ISSA aims to build understanding of AI technologies that are relevant for moving image archives and to explore the potential of these technologies to add public value to screen heritage organisations in their wider contexts.

To do this, we will:

1. Develop a prototype for creative experimentation with moving image collections, including modules for data enrichment, exploration, retrieval, and interaction (DEERIN); 
2. Co-design of situated experimentation workshops delivered through a format called AI for Media Sandbox (AIMS); 
3. Create a publicly accessible code repository and knowledge base for archives to document these experiments and share tools, knowledge and best practice that arises from them; and 
4. Document requirements and sector gaps that can be used to attract future funding and inform strategic decisions about AI in moving image archives. 

---

<!-- slide bg="#f4f4f4" -->

![[ISSA_roadmap.png]]

---


<!-- slide bg="#304f5e" -->

![](fas fa-tv fa-5x) <!-- .element: color="teal" -->

# 2. Antecedentes


---


![[media/anatomy_of_ai.png|1500]]

note:
This was a project in experimental television, in hindsight it was very much of its time, and frankly, as time goes by I am more and more surprised that no one stopped us from doing it. 

In light of everything that happened after with AI, this project stands a somewhat of a quaint expression of many of the issues discussed yesterday, from access and copyright, to the role of generative technologies and their impact on screen culture today.

---


![[media/183e9e0918ae84bbdba0e566dd06a5e7_MD5.jpg|1200]]

note:
A project in 2018 to "machine-see" the BBC television archive, and "machine-edit" new sequences "television by the meter" ‚Äïwe jokingly referred to it.

Aired on BBC 4 on September 2018, seen by half a million people in the UK.
It included four sections that corresponded to different computational techniques to traverse the archive: including text analysis over subtitled material, object detection, motion estimation (visual energy), and a mixed between the three (what we would call today a multimodal method).

---

<iframe src="https://player.vimeo.com/video/429123060?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" width="1920" height="1080" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media" title="1. Made by Machine: Introduction"></iframe>
note:
The possibilities and limitations of AI models in the context of moving image archives.
Analysis = generation.

---

<split even gap="3">

![[media/599f364553168e0448c2566cb33d0c94_MD5.png|700]]

![[assets/images/bce88512ea3044d00c7c54c1897bade9_MD5.png|Open: CMV_contents.png|700]]


</split>

note:
In my book I focus on moving images: artefacts in and of motion that structure cognitive and affective responses in their audiences.

The first two moments: two chapter from my new book in which I discuss philosophies of time and temporal dynamics in moving images from a computational perspective. 
I will present a very condensed version the these chapters as a theoretical foundation, as well as ongoing research of how these concepts can be applied in practice.

---

<section data-background-video="https://github.com/chavezheras/slides/raw/main/assets/Grandmas_Reading_Glass_1900.mp4"
          data-background-video-loop data-background-video-muted>
    <aside class="notes">
          <p>One part of the answer is that this ability to watch films was learned over time. It was not always the case that we knew how to interpret a close up, for example. On screen you see one of the earliest recordings of a close ups in a film. 
		<p>_Grandma's Reading Glass_ (c.1900) by George Albert Smith.
		<p>At the time, the close up was a technical novelty. These early films are demonstrations of technology more than anything else. Through social exposure and continued use over time, techniques like the close up became formal conventions ‚Äï the pieces in a larger aesthetic apparatus of cinema.
     </aside>
</section>

---


![[media/71efe22f312e3f76e247f72f0e7f3e00_MD5.png|1500]]

note:
Using computer vision, we can enlist computers to see these images in our behalf to find some of the patterns that emerge from conventional narrative techniques. For example the shot-reverse-shot, commonly used to depict conversations between characters. Using face detection, it is possible to get a sense of how cinematic discourse is constructed through editing: from wider shots that establish the relations between characters and their environment, progressively in...

---

![[media/95145a7104d332087db98530f2d0c248_MD5.png|1500]]


note:
...to close ups that show characters' inner states, intentions, and reactions, through their facial expressions. David Bordwell calls this editing style from the general to the particular _analytical editing_.

---
## Shot scale detector

![[media/schot_scale_detector.jpg|1500]]

---
## Shot duration distribution

![[media/shot_histogram.jpeg|1200]]

note:
conventional statistics give overview of this dataset of shots

---

## Shot scale breakdown

![[media/shot_scale_chart.jpeg|1200]]

note:
more sophisticated tools to do this now, like Cineshot deep learning method to find shot scale in films

---

![[assets/images/Pasted image 20241016000527.png|1700]]
Image credit: [Ryan Heuser](https://www.cdh.cam.ac.uk/about/people/dr-ryan-heuser/)

---

![[assets/images/Pasted image 20241016000617.png|1700]]
Image credit: [Ryan Heuser](https://www.cdh.cam.ac.uk/about/people/dr-ryan-heuser/)

---

<!-- slide bg="#304f5e" -->

![](fas fa-tv fa-5x) <!-- .element: color="teal" -->

# 3. Estudios de caso


---

<grid  drag="80 60" drop="center" flow="col">
> [!info] DEERIn 
>Is a **tool for research and a technical blueprint** for guided experimentation. 

We will:
>- Enable archives to experiment with existing AI models in their own contexts and collections.
>- Support archives to build the knowledge and evidence to decide if and how to use AI in their organisations.

We will not:
>- Develop a finished AI platform, product or service.
>- Address all aspects of archival practice.
>- Train new AI models from scratch. 

</grid>

---

<grid  drag="80 60" drop="center" flow="col">
> [!example] DEERIn scope and examples
>1. **Data enrichment**: automated captioning and metadata generation using large language and visual models (LLMs and LVMs)
>2. **Exploration**: Moving image collection navigation and visualisation through projection and clustering techniques (t-SNE UMAP)
>3. **Retrieval**: New techniques for retrieval and discovery through implementations of retrieval augmented generation (RAG and GraphRAG) 
>4. **Interaction**: Bootstrapping data-driven interactive applications for creative exploration and reuse of collections, such as AI-assisted editing or generative retrieval (see for example [playphrase.me](https://www.playphrase.me/)). 
</grid>

note:
The proposed DEERIN prototype will be designed as a modular system, using state-of-the-art open-source libraries and models, to provide a functional blueprint of how to enrich, explore and interact with a large collection of moving images.

DEERIn is not a finished product or service, it is a tool for research and a technical blueprint for guided experimentation. We will not address every aspect of archival practice. We will not train new models from scratch. We will enable archives to experiment with existing state of the art open-source models in their own collections and contexts, to build the knowledge and evidence to decide if and how to use and develop AI products and services.

The complementary AIMS workshops are designed to situate the prototype in different contexts, with budgeted time and resources to identify, sample and reshape collection asynchronously, and focus on creative experimentation and discussion in person. 

---


> [!example]  Estudios de caso ISSA
> 1. Segmentaci√≥n sem√°ntica semi-automatizada 
> 2. Generaci√≥n automatizada de audio descriptivo
> 3. B√∫squeda y recuperaci√≥n multi modal
> 4. Edici√≥n semi-automatizada para reuso creativo


---

<!-- slide bg="#5E4031" -->

![](fas fa-tv fa-5x) <!-- .element: color="#91634C" -->

## 3.1 Segmentaci√≥n sem√°ntica semi-automatizada
---



## Experimentos con videocintas de H2022


<split even gap="2">

![[assets/images/a122d80981e384b79764ad1bf5a59ffb_MD5.png]]

<iframe title="vimeo-player" src="https://player.vimeo.com/video/1099640200?h=5f5805e1d4" width="640" height="360" frameborder="0"    allowfullscreen></iframe>

</split>


---

<!-- slide bg="#5E4031" -->

![](fas fa-tv fa-5x) <!-- .element: color="#91634C" -->

## 3.2 Generaci√≥n automatizada de audio descriptivo

---

## DANTE-AD

![[assets/images/b9495677f4e92c63797cc5d5076de55d_MD5.png]]
[Adrienne Deganutti](https://www.linkedin.com/in/adrienne-deganutti-bb28031b6/),¬†[Simon Hadfield](https://www.surrey.ac.uk/people/simon-hadfield),¬†[Andrew Gilbert](https://andrewjohngilbert.github.io/)

C-CATS Lab University of Surrey

---

![[assets/images/8d9e2328ea5c762dc340bb81396b537e_MD5.png]]
[Adrienne Deganutti](https://www.linkedin.com/in/adrienne-deganutti-bb28031b6/),¬†[Simon Hadfield](https://www.surrey.ac.uk/people/simon-hadfield),¬†[Andrew Gilbert](https://andrewjohngilbert.github.io/)

C-CATS Lab University of Surrey

---

<!-- slide bg="#5E4031" -->

![](fas fa-tv fa-5x) <!-- .element: color="#91634C" -->

## 3.3 B√∫squeda y recuperaci√≥n multi modal

---

## WISE-2

![[assets/images/8547da2885922f896c0d0aee0559e6a0_MD5.png|1000]]
Motor de b√∫squeda multimodal [WISE](https://www.robots.ox.ac.uk/~vgg/software/wise/)

[Abhishek Dutta](mailto:adutta@robots.ox.ac.uk) [Visual Geometry Group (VGG)](https://www.robots.ox.ac.uk/~vgg/) University of Oxford.

note:
https://www.robots.ox.ac.uk/~vgg/software/wise/
https://meru.robots.ox.ac.uk/wise/pass/

---

<!-- slide bg="#5E4031" -->
![](fas fa-tv fa-5x) <!-- .element: color="#91634C" -->

## 3.4 Edici√≥n semi-automatizada para reuso creativo


---


![[assets/images/f147e0b337ed2330668fd23c568251d0_MD5.png]]
K1 Adobe Premiere plugin para autoedici√≥n
[KasparAI](https://kasparai.com/)

note:
K1 Adobe premier plugin for prom-editing

---

[[assets/images/d9186f5ef3c80baab006b20a3d3d5143_MD5.png|Open: Pasted image 20250708134351.png]]
![[assets/images/d9186f5ef3c80baab006b20a3d3d5143_MD5.png]]
[FrameSense](https://github.com/kingsdigitallab/framesense)

---

[[assets/images/eb246b12842e74b89e92a268cfd00055_MD5.png|Open: Pasted image 20250708134447.png]]
![[assets/images/eb246b12842e74b89e92a268cfd00055_MD5.png]]
[FrameSense](https://github.com/kingsdigitallab/framesense)

---


<!-- slide bg="#304f5e" -->

![](fas fa-tv fa-5x) <!-- .element: color="teal" -->

# 4. Producci√≥n cultural en la era de la IA

---
<iframe width="1080" height="768" src="https://www.youtube.com/embed/CNIlqJctA_I?si=hassiNEby7XizlFS" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
note:
El medio es el mensaje. Esto era cierto del radio, el cine y la television.
El medio no es el artefacto, ni sus usuarios, sino la red de relaciones que se construyen cuando artefactos y audiencias entren en contacto. La diferencia clave con tecnolog√≠as de IA es que esta red invisible de relaciones ha sido capturada y abstra√≠da en modelos comunicacionales capaces de reproducir la potencialidad de esta red de relaciones.
El reto con la producci√≥n cultural en la era de la IA es que estos modelos de potencialidades latentes modifican radicalmente el modo de producci√≥n, preservaci√≥n, y acceso, a la cultura.

Benjamin Desai is a creative technologist and digital artist focused on AR and cofounder of Radical Realities, an award-winning immersive studio.  ‚ÄúExcited to share this imaginative look into an alternate past powered by Sora. Blending early 20th-century film aesthetics with whimsical scenarios and placing animals in unexpected roles. This work aims to ignite a sense of wonder while showcasing the potential of today's technology. Creating with Sora is still an experimental process, involving a lot of iteration and fine-tuning. It's much more of a human-AI collaboration than a magic button solution. Check out these fascinating animals and the joy of asking 'what if?'‚Äù

Analytic and generative approaches in computing tend to be split between scientific and creative domains, with their respective tools and communities of practice.

Deployed as analytical engines, computers can be used to find patterns across vast collections of imagery, and these patterns are often expressed as relations of proximity in space. We have seen examples of this yesterday in multiple way of projecting archives onto 2d or 3d spaces.

The image in this slide shows a t-SNE mapping of a collection of soviet news reels. The dataset and the tool were developed by colleagues in CUDAN from the cultural data analytics lab in Tallinn, Estonia.

But to amount to knowledge, these spatial correlations require interpretation and explanation, which require relations of necessity, not just proximity, and tend to unfold sequentially, as researchers, critics and users seek to organise these patterns to infer causal relations and plausible reasons for data objects and events to be organised in space the way they are.

By coupling an analytical engine with a generative one, computing can be used to configure narratives about these proximity patterns and enable explanatory propositions through compositional techniques familiar to media scholarship.

---

![[assets/images/ISSA_web_snap.png|1000]]
[BFI press release](https://core-cms.bfi.org.uk/media/39909/download "https://core-cms.bfi.org.uk/media/39909/download")  ¬∑  [ISSA website](https://www.kcl.ac.uk/research/issa "https://www.kcl.ac.uk/research/issa")  

---

![[ISSA_FAQ 1.png|1000]]
[ISSA code repository and WIKI](https://github.com/kingsdigitallab/issa/wiki/Frequently-Asked-Questions)


---


<!-- slide bg="#304f5e" -->

![](fas fa-tv fa-5x) <!-- .element: color="teal" -->

# 5. Preguntas

---

