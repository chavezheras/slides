---
title: ISSA-SEPT2025
draft: false
tags:
  - presentation
date_created: 15 September 2025
date_modified: 15 September 2025
margin: 0
width: 1920
height: 1200
transition: slide
theme: black
---


![[ISSA_1.png]]
### FAUK, September 2025

---

![[assets/images/ISSA_web_snap.png|1000]]
[BFI press release](https://core-cms.bfi.org.uk/media/39909/download "https://core-cms.bfi.org.uk/media/39909/download")  ·  [ISSA website](https://www.kcl.ac.uk/research/issa "https://www.kcl.ac.uk/research/issa")  

note:
To drive **creative experimentation with AI technologies** in the screen heritage sector by developing the knowledge, tools, and skills needed to rethink large audiovisual collections from a computational perspective. > > ISSA aims to build understanding of AI technologies that are relevant for moving image archives and to explore the potential of these technologies to add public value to screen heritage organisations in their wider contexts.

--

<grid  drag="80 60" drop="center" flow="col">

> [!info]  Why ISSA?
> To drive **creative experimentation with AI technologies** in the screen heritage sector by developing the knowledge, tools, and skills needed to rethink large audiovisual collections from a computational perspective. 
> 
> ISSA aims to build understanding of AI technologies that are relevant for moving image archives and to explore the potential of these technologies to add public value to screen heritage organisations in their wider contexts.
</grid>

note:
Collaboration and deeper integration is the path forward

---
<!-- slide bg="#f4f4f4" -->

![[assets/images/ISSA_partners.png|1200]]

--

<!-- slide bg="#f4f4f4" -->

![[ISSA_summary.png]]

note:

Daniel
### Aims and objectives 
To drive **creative experimentation with AI technologies** in the screen heritage sector by developing the knowledge, tools, and skills needed to rethink large audiovisual collections from a computational perspective. 

ISSA aims to build understanding of AI technologies that are relevant for moving image archives and to explore the potential of these technologies to add public value to screen heritage organisations in their wider contexts.

To do this, we will:

1. Develop a prototype for creative experimentation with moving image collections, including modules for data enrichment, exploration, retrieval, and interaction (DEERIN); 
2. Co-design of situated experimentation workshops delivered through a format called AI for Media Sandbox (AIMS); 
3. Create a publicly accessible code repository and knowledge base for archives to document these experiments and share tools, knowledge and best practice that arises from them; and 
4. Document requirements and sector gaps that can be used to attract future funding and inform strategic decisions about AI in moving image archives. 

### Approach
ISSA adopts a human-centred design approach to technology development. In practice, this means prioritising the needs, preferences and expectations of end-users and stake holders, such as archivists, curators, and the different publics who engage with audiovisual collections. The goal is not to suggest technical fixes to existing archiving and preservation challenges, but to collectively rethink the archive and its practices from a computational perspective.

---

<!-- slide bg="#f4f4f4" -->

![[ISSA_roadmap.png]]

note:
## Milestones and weekly breakdown (first phase)

- Soft launch and partner engagement \[15 January – 28 February 2025] 
	- Initial engagement with archive partners and stakeholders 
		- Project website and FAQs 
		- Sot launch webinar for partners (internal, online) 
		- Soft launch town hall (external, online) 

- R&D consultation and diagnostic \[1 March – 31 June 2025] 
	- Project setup on the system, cost codes created 
		- Archive partners are setup as King’s suppliers 
		- Access to the project shared platform 
		- Setup of project mailing list and comms channels 
		- Collections and technical audit (per partner, remote) 

- Development of DEERIN \[7 July – 19 December 2025] 
	- R&D partners design and develop DEERIN
	- FIAT/IFTA conference * 
	- R&D develop and test sandbox environment. 

- Inception demonstrator \[5-23 January 2026] 
	- Project inception demonstrator, joint event at KCL (~20-25 participants). R&D partners present the DEERIN demo and sandbox environment
	- Archive partners agree on access conditions; sample size, preprocessing needed, and data transfer protocol. 

---


> [!question]  Archive engagement
> 15 interviews, between March and August
> Tech review

![](fas fa-arrow-circle-down fa-2x) <!-- .element: color="#a9a9a9" -->

> [!tod]  Four use cases
> 1. Semantic segmentation at scale 
> 2. Place-based search and retrieval
> 3. Automated audio description
> 4. Algorithmic editing for creative re-use



note:
More details in Wiki

---

<!-- slide bg="#5E4031" -->

![](fas fa-cube fa-5x) <!-- .element: color="#91634C" -->

## 1. Semantic segmentation at scale 

Using LVMs to segment long-form video files into meaningful smaller units

note:
Collaboration and deeper integration is the path forward


---

## Digitised Videotapes from H2022


<split even gap="2">

![[assets/images/a122d80981e384b79764ad1bf5a59ffb_MD5.png]]

![[media/UTUV_test_Hawthorne.png|500]]

</split>

---

<!-- slide bg="#5E4031" -->

![](fas fa-cube fa-5x) <!-- .element: color="#91634C" -->

## 2. Place-based search and retrieval

Using multi-modal embeddings to surface local and regional references in archive material 

---

## WISE-2

![[assets/images/8547da2885922f896c0d0aee0559e6a0_MD5.png|1000]]
 [WISE](https://www.robots.ox.ac.uk/~vgg/software/wise/) search engine. [Abhishek Dutta](mailto:adutta@robots.ox.ac.uk),  [Visual Geometry Group (VGG)](https://www.robots.ox.ac.uk/~vgg/) , University of Oxford.

note:
https://www.robots.ox.ac.uk/~vgg/software/wise/
https://meru.robots.ox.ac.uk/wise/pass/

---

<!-- slide bg="#5E4031" -->
![](fas fa-cube fa-5x) <!-- .element: color="#91634C" -->

## 3. Automated audio description

Using specialised open-source models to generate audio descriptions of archive material

---
## DANTE-AD

![[assets/images/8d9e2328ea5c762dc340bb81396b537e_MD5.png]]
[Adrienne Deganutti](https://www.linkedin.com/in/adrienne-deganutti-bb28031b6/), [Simon Hadfield](https://www.surrey.ac.uk/people/simon-hadfield), [Andrew Gilbert](https://andrewjohngilbert.github.io/)

C-CATS Lab University of Surrey

--
## DANTE-AD

![[assets/images/b9495677f4e92c63797cc5d5076de55d_MD5.png]]
[Adrienne Deganutti](https://www.linkedin.com/in/adrienne-deganutti-bb28031b6/), [Simon Hadfield](https://www.surrey.ac.uk/people/simon-hadfield), [Andrew Gilbert](https://andrewjohngilbert.github.io/)

C-CATS Lab University of Surrey

---

<!-- slide bg="#5E4031" -->
![](fas fa-cube fa-5x) <!-- .element: color="#91634C" -->

## 4. Algorithmic editing and creative re-use 

Combine the outputs and techniques from 1-3 to prototype interfaces for creative re-use

---

<split even gap="2">

![](https://lav.io/notes/videogrep-tutorial/importxml_huc90048f0937812a2d1ff1c7b33827d46_276754_1000x0_resize_box_3.png)


> [!example]  [Videogrep](https://github.com/antiboredom/videogrep) by [Sam Lavigne](https://lav.io/)
> - [silence extraction](https://github.com/antiboredom/videogrep/blob/master/examples/only_silence.py)
> - [automatically creating supercuts](https://github.com/antiboredom/videogrep/blob/master/examples/auto_supercut.py)
> - [supercuts based on youtube searches](https://github.com/antiboredom/videogrep/blob/master/examples/auto_youtube.py)
> - [supercuts from specific parts of speech](https://github.com/antiboredom/videogrep/blob/master/examples/parts_of_speech.py)
> - [supercuts from spacy pattern matching](https://github.com/antiboredom/videogrep/blob/master/examples/pattern_matcher.py)

</split>


note:
Videogrep is a command line tool that searches through dialog in video or audio files and makes supercuts based on what it finds. It will recognize `.srt` or `.vtt` subtitle tracks, or transcriptions that can be generated with vosk, pocketsphinx, and other tools.
Ideal for https://www.archivesforeducation.com/

--

![[assets/images/f147e0b337ed2330668fd23c568251d0_MD5.png]]
K1 Adobe Premiere plugin para autoedición
[KasparAI](https://kasparai.com/)

note:
K1 Adobe premier plugin for prom-editing

--

![[media/playphrase_me.png.png]]
[PlayPhrase.me](https://www.playphrase.me/)

---


<!-- slide bg="#5E4031" -->
![](fas fa-cubes fa-5x) <!-- .element: color="#91634C" -->

##  Connecting tools with use cases 

A middleware layer to reshape moving image collections into data that is useful for tool development

---

![[assets/images/frame_sense_screenshot.png|1000]]
[FrameSense](https://github.com/kingsdigitallab/framesense)

---

![[assets/images/frame_sense_diagram.png]]
[FrameSense](https://github.com/kingsdigitallab/framesense)

---

> [!example]  Next steps 2025
> - MVPs for each use case
> - Licencing agreements for data sharing
> - Technical solution for data sharing
> - Eliciting data from archive partners
> - Prototype development

> [!tip]  Looking forward to 2026
> - Prototype demonstrator event in February!
> - AIMS workshops


note:
More details in Wiki

---


<!-- slide bg="#992d51" -->
## Get in touch

![](fas fa-envelope fa-3x)<!-- .element: color="#b5788d"--> 
 issa@kcl.ac.uk

![[assets/images/qr_issa_web.png]]


---
